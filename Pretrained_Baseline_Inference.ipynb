{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Brain-to-Text '25 - Pretrained Baseline Inference\n",
                "\n",
                "This notebook uses the **pretrained RNN baseline** (6-12% PER) for inference.\n",
                "\n",
                "**Steps:**\n",
                "1. Upload data + checkpoint to Drive\n",
                "2. Run inference on test set\n",
                "3. Generate submission CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q jiwer pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# UPDATE THESE PATHS\n",
                "DATA_DIR = '/content/drive/MyDrive/hdf5_data_final'\n",
                "CHECKPOINT_PATH = '/content/drive/MyDrive/t15_pretrained_rnn_baseline/t15_pretrained_rnn_baseline/checkpoint/best_checkpoint'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import h5py\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "from scipy.ndimage import gaussian_filter1d\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f'Device: {device}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Configuration (from args.yaml)\n",
                "class CFG:\n",
                "    n_input_features = 512\n",
                "    n_units = 768\n",
                "    n_layers = 5\n",
                "    n_classes = 41\n",
                "    rnn_dropout = 0.4\n",
                "    input_layer_dropout = 0.2\n",
                "    patch_size = 14\n",
                "    patch_stride = 4\n",
                "    smooth_kernel_std = 2\n",
                "\n",
                "# Session mapping - must match training order!\n",
                "SESSIONS = [\n",
                "    't15.2023.08.11', 't15.2023.08.13', 't15.2023.08.18', 't15.2023.08.20',\n",
                "    't15.2023.08.25', 't15.2023.08.27', 't15.2023.09.01', 't15.2023.09.03',\n",
                "    't15.2023.09.24', 't15.2023.09.29', 't15.2023.10.01', 't15.2023.10.06',\n",
                "    't15.2023.10.08', 't15.2023.10.13', 't15.2023.10.15', 't15.2023.10.20',\n",
                "    't15.2023.10.22', 't15.2023.11.03', 't15.2023.11.04', 't15.2023.11.17',\n",
                "    't15.2023.11.19', 't15.2023.11.26', 't15.2023.12.03', 't15.2023.12.08',\n",
                "    't15.2023.12.10', 't15.2023.12.17', 't15.2023.12.29', 't15.2024.02.25',\n",
                "    't15.2024.03.03', 't15.2024.03.08', 't15.2024.03.15', 't15.2024.03.17',\n",
                "    't15.2024.04.25', 't15.2024.04.28', 't15.2024.05.10', 't15.2024.06.14',\n",
                "    't15.2024.07.19', 't15.2024.07.21', 't15.2024.07.28', 't15.2025.01.10',\n",
                "    't15.2025.01.12', 't15.2025.03.14', 't15.2025.03.16', 't15.2025.03.30',\n",
                "    't15.2025.04.13'\n",
                "]\n",
                "SESSION_TO_ID = {s: i for i, s in enumerate(SESSIONS)}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GRU Decoder Model (exact architecture from training log)\n",
                "class GRUDecoder(nn.Module):\n",
                "    def __init__(self, n_days=45):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Day-specific layers (adapters)\n",
                "        self.day_weights = nn.ParameterList([\n",
                "            nn.Parameter(torch.eye(CFG.n_input_features)) for _ in range(n_days)\n",
                "        ])\n",
                "        self.day_biases = nn.ParameterList([\n",
                "            nn.Parameter(torch.zeros(1, CFG.n_input_features)) for _ in range(n_days)\n",
                "        ])\n",
                "        self.day_layer_activation = nn.Softsign()\n",
                "        self.day_layer_dropout = nn.Dropout(CFG.input_layer_dropout)\n",
                "        \n",
                "        # GRU input size = 512 * 14 = 7168 (patch_size=14)\n",
                "        gru_input_size = CFG.n_input_features * CFG.patch_size\n",
                "        \n",
                "        self.gru = nn.GRU(\n",
                "            input_size=gru_input_size,\n",
                "            hidden_size=CFG.n_units,\n",
                "            num_layers=CFG.n_layers,\n",
                "            batch_first=True,\n",
                "            dropout=CFG.rnn_dropout\n",
                "        )\n",
                "        self.out = nn.Linear(CFG.n_units, CFG.n_classes)\n",
                "    \n",
                "    def forward(self, x, day_idx):\n",
                "        # x: [B, T, 512]\n",
                "        B, T, D = x.shape\n",
                "        \n",
                "        # Apply day-specific transformation\n",
                "        W = self.day_weights[day_idx]  # [512, 512]\n",
                "        b = self.day_biases[day_idx]   # [1, 512]\n",
                "        x = torch.matmul(x, W) + b\n",
                "        x = self.day_layer_activation(x)\n",
                "        x = self.day_layer_dropout(x)\n",
                "        \n",
                "        # Patch embedding: unfold with patch_size=14, patch_stride=4\n",
                "        # Creates overlapping patches\n",
                "        patches = x.unfold(1, CFG.patch_size, CFG.patch_stride)  # [B, num_patches, D, patch_size]\n",
                "        patches = patches.permute(0, 1, 3, 2)  # [B, num_patches, patch_size, D]\n",
                "        patches = patches.reshape(B, patches.size(1), -1)  # [B, num_patches, patch_size*D]\n",
                "        \n",
                "        # GRU\n",
                "        out, _ = self.gru(patches)\n",
                "        logits = self.out(out)\n",
                "        \n",
                "        return F.log_softmax(logits, dim=-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phoneme vocabulary\n",
                "VOCAB = ['', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER',\n",
                "         'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW',\n",
                "         'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH', '|']\n",
                "\n",
                "def greedy_decode(logits):\n",
                "    \"\"\"CTC greedy decoding\"\"\"\n",
                "    pred = logits.argmax(dim=-1)  # [T]\n",
                "    result = []\n",
                "    prev = -1\n",
                "    for p in pred:\n",
                "        if p != prev and p != 0:  # Skip blanks and repeats\n",
                "            result.append(VOCAB[p])\n",
                "        prev = p\n",
                "    return ' '.join(result)\n",
                "\n",
                "def preprocess(x, sigma=2):\n",
                "    \"\"\"Gaussian smoothing\"\"\"\n",
                "    if sigma > 0:\n",
                "        x = gaussian_filter1d(x, sigma=sigma, axis=0)\n",
                "    return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "print('Loading pretrained model...')\n",
                "model = GRUDecoder(n_days=len(SESSIONS)).to(device)\n",
                "\n",
                "# Load checkpoint\n",
                "ckpt = torch.load(CHECKPOINT_PATH, map_location=device)\n",
                "\n",
                "# Handle torch.compile wrapper\n",
                "if 'model_state_dict' in ckpt:\n",
                "    state_dict = ckpt['model_state_dict']\n",
                "else:\n",
                "    state_dict = ckpt\n",
                "\n",
                "# Remove '_orig_mod.' prefix if present (from torch.compile)\n",
                "new_state_dict = {}\n",
                "for k, v in state_dict.items():\n",
                "    if k.startswith('_orig_mod.'):\n",
                "        new_state_dict[k[10:]] = v\n",
                "    else:\n",
                "        new_state_dict[k] = v\n",
                "\n",
                "model.load_state_dict(new_state_dict, strict=False)\n",
                "model.eval()\n",
                "print(f'Model loaded! {sum(p.numel() for p in model.parameters()):,} parameters')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run inference on test set\n",
                "print('Running inference on test data...')\n",
                "results = []\n",
                "\n",
                "for session_folder in tqdm(sorted(os.listdir(DATA_DIR))):\n",
                "    session_path = os.path.join(DATA_DIR, session_folder)\n",
                "    if not os.path.isdir(session_path):\n",
                "        continue\n",
                "    \n",
                "    test_file = os.path.join(session_path, 'data_test.hdf5')\n",
                "    if not os.path.exists(test_file):\n",
                "        continue\n",
                "    \n",
                "    # Get session ID\n",
                "    session_id = SESSION_TO_ID.get(session_folder, 0)\n",
                "    \n",
                "    with h5py.File(test_file, 'r') as f:\n",
                "        for trial_key in sorted(f.keys()):\n",
                "            # Load and preprocess\n",
                "            x = f[trial_key]['input_features'][:]\n",
                "            x = preprocess(x, sigma=CFG.smooth_kernel_std)\n",
                "            x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(device)\n",
                "            \n",
                "            # Inference\n",
                "            with torch.no_grad():\n",
                "                logits = model(x, session_id)\n",
                "            \n",
                "            # Decode\n",
                "            pred_text = greedy_decode(logits[0])\n",
                "            \n",
                "            results.append({\n",
                "                'id': f'{session_folder}_{trial_key}',\n",
                "                'transcription': pred_text\n",
                "            })\n",
                "\n",
                "print(f'Generated {len(results)} predictions')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create submission\n",
                "df = pd.DataFrame(results)\n",
                "print(f'Submission shape: {df.shape}')\n",
                "print(df.head(10))\n",
                "\n",
                "# Save\n",
                "df.to_csv('/content/submission.csv', index=False)\n",
                "print('\\nSaved to /content/submission.csv')\n",
                "print('\\nDownload this file and submit to the competition!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optional: View sample predictions\n",
                "print('\\nSample predictions:')\n",
                "for i in range(min(5, len(results))):\n",
                "    print(f\"  {results[i]['id']}: {results[i]['transcription'][:80]}...\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}