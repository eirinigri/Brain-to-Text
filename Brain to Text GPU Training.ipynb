{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ28joeLB9R5"
      },
      "source": [
        "# Brain-to-Text '25 - GPU Training\n",
        "This notebook trains the Conformer model with k-fold cross-validation.\n",
        "\n",
        "**Instructions:**\n",
        "1. Upload your data to Google Drive\n",
        "2. Mount Drive and update `DATA_DIR` path\n",
        "3. Run all cells\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HsB17cVB9R7",
        "outputId": "99afe8da-3a80-49e0-ac94-aad166295d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install essential libraries for evaluation and decoding\n",
        "!pip install -q jiwer scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kDba5fWB9R9",
        "outputId": "5b61ded0-d32d-4323-b6c5-730bf853d58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import h5py\n",
        "import torch\n",
        "import numpy as np\n",
        "import jiwer\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.nn import functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.amp import autocast, GradScaler\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from sklearn.model_selection import KFold\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set memory alloc for fragmentation handling\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "class CFG:\n",
        "    # --- Paths ---\n",
        "    DATA_DIR = '/content/drive/MyDrive/hdf5_data_final'\n",
        "\n",
        "    # --- Training Params ---\n",
        "    N_FOLDS = 4            # Increased to 4 for better validation stability\n",
        "    EPOCHS = 10            # Enough for convergence with pre-trained initialization\n",
        "    LR = 8e-4              # Slightly lower LR for stability with adapters\n",
        "    BATCH_SIZE = 8         # Conservative batch size to fit in VRAM with deep model\n",
        "    GRAD_ACCUM_STEPS = 4   # Effective batch size = 32 (8 * 4)\n",
        "\n",
        "    # --- Model Architecture ---\n",
        "    INPUT_DIM = 512\n",
        "    ENCODER_DIM = 256\n",
        "    N_LAYERS = 6           # Deeper model (enabled by gradient checkpointing)\n",
        "    N_HEAD = 4\n",
        "    OUTPUT_DIM = 41        # 40 phonemes + 1 blank\n",
        "\n",
        "    # --- Signal Processing ---\n",
        "    SMOOTHING_SIGMA = 20   # Gaussian smoothing factor\n",
        "    ROBUST_SCALING = True  # Use Median/IQR scaling\n",
        "\n",
        "    # --- Hardware ---\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f\"Running on {CFG.DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "84QAP-GXB9SB",
        "outputId": "e0ec684d-0c55-45fe-bee1-29325971a9f8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1469083407.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjiwer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgaussian_filter1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Check GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_arraylike_not_scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_numpy_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_preserve_dia_indices_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0m_NUMPY_NAMESPACE_NAMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array_api_compat.numpy\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    604\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 606\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_mstats_basic\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_discrete_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_continuous_distns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/stats/_discrete_distns.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetaln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammaln\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgamln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog1p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msinh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/interpolate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_interpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_fitpack_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/interpolate/_interpolate.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fitpack_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdfitpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_polyint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Interpolator1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/interpolate/_fitpack_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# These are in the API for fitpack even if not used in fitpack.py itself.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_fitpack_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbisplrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbisplev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdblint\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fitpack_impl\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bsplines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBSpline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/interpolate/_fitpack_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m _parcur_cache = {'t': array([], float), 'wrk': array([], float),\n\u001b[0;32m--> 103\u001b[0;31m                  \u001b[0;34m'iwrk'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfitpack_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'u'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                  'ub': 0, 'ue': 1}\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_construction_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_align\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalignedstruct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", align=True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_construction_repr\u001b[0;34m(dtype, include_align, short)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_subarray_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_scalar_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_scalar_str\u001b[0;34m(dtype, short)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"'%sm8%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbyteorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_datetime_metadata_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Short repr with endianness, like '<f8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshort\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteorder\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \"\"\"\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0marg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 4 frames repeated, from the frame below ...\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_construction_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_align\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalignedstruct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", align=True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ],
      "source": [
        "# --- Helper Functions ---\n",
        "def gaussian_smoothing(data, sigma=20):\n",
        "    return gaussian_filter1d(data, sigma=sigma, axis=0)\n",
        "\n",
        "def robust_scale_trial(data):\n",
        "    \"\"\"\n",
        "    Scales data using Median and Interquartile Range (IQR).\n",
        "    Robust to high-amplitude artifacts/spikes.\n",
        "    \"\"\"\n",
        "    # Calculate stats per channel across time\n",
        "    median = np.median(data, axis=0)\n",
        "    q75, q25 = np.percentile(data, [75, 25], axis=0)\n",
        "    iqr = q75 - q25\n",
        "    # Avoid division by zero\n",
        "    iqr[iqr == 0] = 1.0\n",
        "    return (data - median) / iqr\n",
        "\n",
        "# --- Dataset Class ---\n",
        "class BrainDataset(Dataset):\n",
        "    def __init__(self, hdf5_files, session_mapping, is_test=False, train_mode=True):\n",
        "        self.files = []\n",
        "        self.session_ids = []\n",
        "        self.keys = []\n",
        "\n",
        "        # Pre-scan files to build index\n",
        "        for f_path in hdf5_files:\n",
        "            sess_name = os.path.basename(os.path.dirname(f_path))\n",
        "            sid = session_mapping.get(sess_name, 0)\n",
        "            with h5py.File(f_path, 'r') as f:\n",
        "                keys = sorted(list(f.keys()))\n",
        "                for k in keys:\n",
        "                    self.files.append(f_path)\n",
        "                    self.session_ids.append(sid)\n",
        "                    self.keys.append(k)\n",
        "\n",
        "        self.is_test = is_test\n",
        "        self.train_mode = train_mode\n",
        "        self.opened_files = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        f_path = self.files[idx]\n",
        "        if f_path not in self.opened_files:\n",
        "            self.opened_files[f_path] = h5py.File(f_path, 'r')\n",
        "\n",
        "        key = self.keys[idx]\n",
        "        grp = self.opened_files[f_path][key]\n",
        "\n",
        "        # 1. Load Input\n",
        "        x = grp['input_features'][:]\n",
        "\n",
        "        # 2. Apply Signal Processing\n",
        "        if CFG.SMOOTHING_SIGMA > 0:\n",
        "            x = gaussian_smoothing(x, CFG.SMOOTHING_SIGMA)\n",
        "        if CFG.ROBUST_SCALING:\n",
        "            x = robust_scale_trial(x)\n",
        "\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "        # 3. Load Targets\n",
        "        y = torch.tensor(grp['seq_class_ids'][:], dtype=torch.long)\n",
        "\n",
        "        return x, y, self.session_ids[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    xs, ys, sids = zip(*batch)\n",
        "\n",
        "    # Safety: Ensure CTC constraints (Target Length <= Input Length)\n",
        "    xs_safe, ys_safe = [], []\n",
        "    for x, y in zip(xs, ys):\n",
        "        if len(y) > len(x):\n",
        "            y = y[:len(x)] # Truncate target if longer than input\n",
        "        xs_safe.append(x)\n",
        "        ys_safe.append(y)\n",
        "\n",
        "    x_lens = torch.tensor([len(x) for x in xs_safe], dtype=torch.long)\n",
        "    y_lens = torch.tensor([len(y) for y in ys_safe], dtype=torch.long)\n",
        "    sids = torch.tensor(sids, dtype=torch.long)\n",
        "\n",
        "    padded_x = rnn_utils.pad_sequence(xs_safe, batch_first=True)\n",
        "    padded_y = rnn_utils.pad_sequence(ys_safe, batch_first=True)\n",
        "\n",
        "    return padded_x, padded_y, x_lens, y_lens, sids\n",
        "\n",
        "# --- Loader Utility ---\n",
        "def get_all_filepaths(data_dir):\n",
        "    train_files = []\n",
        "    sessions = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
        "    sess_map = {name: i for i, name in enumerate(sessions)}\n",
        "\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for f in files:\n",
        "            if f == 'data_train.hdf5' or f == 'data_val.hdf5': # Use both for Cross Validation\n",
        "                train_files.append(os.path.join(root, f))\n",
        "\n",
        "    return train_files, sess_map, len(sessions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VMT63k8B9SD"
      },
      "outputs": [],
      "source": [
        "# --- Model Components ---\n",
        "class SubjectAdapter(nn.Module):\n",
        "    \"\"\"Learns a unique projection for each day/session to normalize inputs\"\"\"\n",
        "    def __init__(self, dim, n_sess):\n",
        "        super().__init__()\n",
        "        self.w = nn.Parameter(torch.stack([torch.eye(dim) for _ in range(n_sess)]))\n",
        "        self.b = nn.Parameter(torch.zeros(n_sess, dim))\n",
        "\n",
        "    def forward(self, x, sids):\n",
        "        # Batch Matrix Multiply: x[B,T,D] @ w[B,D,D] + b[B,1,D]\n",
        "        return torch.bmm(x, self.w[sids]) + self.b[sids].unsqueeze(1)\n",
        "\n",
        "class ConformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_head, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(dim)\n",
        "        # Flash Attention (Scalable Dot Product)\n",
        "        self.attn = nn.MultiheadAttention(dim, n_head, dropout=drop, batch_first=True)\n",
        "        self.ln2 = nn.LayerNorm(dim)\n",
        "        self.ff1 = nn.Sequential(\n",
        "            nn.Linear(dim, dim*4), nn.SiLU(), nn.Dropout(drop), nn.Linear(dim*4, dim)\n",
        "        )\n",
        "        self.ln3 = nn.LayerNorm(dim)\n",
        "        # Conv Module\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(dim, dim*2, 1), nn.GLU(dim=1),\n",
        "            nn.Conv1d(dim, dim, 31, padding=15, groups=dim), # Depthwise\n",
        "            nn.BatchNorm1d(dim), nn.SiLU(),\n",
        "            nn.Conv1d(dim, dim, 1), nn.Dropout(drop)\n",
        "        )\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. FF (Macaron style half-step)\n",
        "        x = x + 0.5 * self.ff1(self.ln1(x))\n",
        "        # 2. Attention\n",
        "        res = x\n",
        "        x_norm = self.ln2(x)\n",
        "        # Using PyTorch 2.0+ optimized attention\n",
        "        att_out, _ = self.attn(x_norm, x_norm, x_norm, need_weights=False)\n",
        "        x = res + self.drop(att_out)\n",
        "        # 3. Conv\n",
        "        res = x\n",
        "        x_norm = self.ln3(x).transpose(1, 2) # [B, D, T]\n",
        "        conv_out = self.conv(x_norm).transpose(1, 2) # [B, T, D]\n",
        "        x = res + conv_out\n",
        "        return x\n",
        "\n",
        "class BrainToTextModel(nn.Module):\n",
        "    def __init__(self, num_sessions):\n",
        "        super().__init__()\n",
        "        # Phase 2: Adapter\n",
        "        self.adapter = SubjectAdapter(CFG.INPUT_DIM, num_sessions)\n",
        "        self.inp_proj = nn.Linear(CFG.INPUT_DIM, CFG.ENCODER_DIM)\n",
        "\n",
        "        # Deep Encoder with Gradient Checkpointing\n",
        "        self.layers = nn.ModuleList([\n",
        "            ConformerBlock(CFG.ENCODER_DIM, CFG.N_HEAD) for _ in range(CFG.N_LAYERS)\n",
        "        ])\n",
        "\n",
        "        self.out_proj = nn.Linear(CFG.ENCODER_DIM, CFG.OUTPUT_DIM)\n",
        "\n",
        "    def forward(self, x, sids):\n",
        "        x = self.adapter(x, sids)\n",
        "        x = self.inp_proj(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # Checkpointing saves VRAM allowing larger batches/deeper models\n",
        "            x = checkpoint(layer, x, use_reentrant=False)\n",
        "\n",
        "        return F.log_softmax(self.out_proj(x), dim=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPQ_TrLuB9SE"
      },
      "outputs": [],
      "source": [
        "VOCAB = ['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER',\n",
        "         'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW',\n",
        "         'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH', '|']\n",
        "TOKEN_MAP = {i + 1: phoneme for i, phoneme in enumerate(VOCAB)}\n",
        "TOKEN_MAP[0] = \"\"\n",
        "\n",
        "def beam_search_decode(log_probs, beam_width=5):\n",
        "    \"\"\"\n",
        "    Pure Python CTC Beam Search for validation.\n",
        "    \"\"\"\n",
        "    T, C = log_probs.shape\n",
        "    # Beam: {prefix_tuple: (prob_blank, prob_non_blank)}\n",
        "    beams = {(): (0.0, float('-inf'))}\n",
        "\n",
        "    for t in range(T):\n",
        "        next_beams = defaultdict(lambda: (float('-inf'), float('-inf')))\n",
        "        for prefix, (p_b, p_nb) in beams.items():\n",
        "            p_total = np.logaddexp(p_b, p_nb)\n",
        "            if p_total < -20: continue # Pruning low prob paths\n",
        "\n",
        "            for c in range(C):\n",
        "                p_c = log_probs[t, c]\n",
        "                if c == 0: # Blank\n",
        "                    n_pb, n_pnb = next_beams[prefix]\n",
        "                    n_pb = np.logaddexp(n_pb, p_total + p_c)\n",
        "                    next_beams[prefix] = (n_pb, n_pnb)\n",
        "                else: # Non-blank\n",
        "                    end_t = prefix[-1] if prefix else None\n",
        "                    new_prefix = prefix + (c,)\n",
        "\n",
        "                    n_pb, n_pnb = next_beams[new_prefix]\n",
        "\n",
        "                    if c == end_t: # Repeated char\n",
        "                        # Don't extend, just update probability of current prefix ending in non-blank\n",
        "                        # This part handles the \"aa\" vs \"a\" logic in CTC\n",
        "                        prev_pb, prev_pnb = next_beams[prefix]\n",
        "                        prev_pnb = np.logaddexp(prev_pnb, p_nb + p_c)\n",
        "                        next_beams[prefix] = (prev_pb, prev_pnb)\n",
        "                        # Extend via blank\n",
        "                        n_pnb = np.logaddexp(n_pnb, p_b + p_c)\n",
        "                    else:\n",
        "                        n_pnb = np.logaddexp(n_pnb, p_total + p_c)\n",
        "\n",
        "                    next_beams[new_prefix] = (n_pb, n_pnb)\n",
        "\n",
        "        # Sort and keep top K\n",
        "        sorted_beams = sorted(\n",
        "            next_beams.items(),\n",
        "            key=lambda k: np.logaddexp(*k[1]),\n",
        "            reverse=True\n",
        "        )\n",
        "        beams = dict(sorted_beams[:beam_width])\n",
        "\n",
        "    best_prefix = max(beams.keys(), key=lambda k: np.logaddexp(*beams[k]))\n",
        "    return \" \".join([TOKEN_MAP.get(i, \"\") for i in best_prefix])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0ENN6VXB9SE"
      },
      "outputs": [],
      "source": [
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    preds, trues = [], []\n",
        "    crit = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y, xl, yl, sids in tqdm(loader, desc=\"Validating\", leave=False):\n",
        "            x, y = x.to(CFG.DEVICE), y.to(CFG.DEVICE)\n",
        "            sids = sids.to(CFG.DEVICE)\n",
        "\n",
        "            with autocast('cuda', dtype=torch.bfloat16):\n",
        "                logits = model(x, sids)\n",
        "                loss = crit(logits.permute(1,0,2), y, xl, yl)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            # Decode a few for WER\n",
        "            cpu_logits = logits.float().cpu().numpy()\n",
        "            for i in range(len(x)):\n",
        "                # Use Beam Search for better accuracy\n",
        "                p_text = beam_search_decode(cpu_logits[i, :xl[i]], beam_width=3)\n",
        "                t_text = \" \".join([TOKEN_MAP.get(k.item(), \"\") for k in y[i, :yl[i]]])\n",
        "                preds.append(p_text)\n",
        "                trues.append(t_text)\n",
        "\n",
        "    wer = jiwer.wer(trues, preds)\n",
        "    return np.mean(losses), wer\n",
        "\n",
        "# --- Main Execution ---\n",
        "files, sess_map, n_sess = get_all_filepaths(CFG.DATA_DIR)\n",
        "print(f\"Found {len(files)} files across {n_sess} sessions.\")\n",
        "\n",
        "kfold = KFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(files)):\n",
        "    print(f\"\\n{'='*20} FOLD {fold+1}/{CFG.N_FOLDS} {'='*20}\")\n",
        "\n",
        "    # 1. Setup Data\n",
        "    train_ds = BrainDataset([files[i] for i in train_idx], sess_map)\n",
        "    val_ds = BrainDataset([files[i] for i in val_idx], sess_map)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=CFG.BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "    # 2. Setup Model & Optim\n",
        "    model = BrainToTextModel(n_sess).to(CFG.DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=CFG.LR, weight_decay=1e-2)\n",
        "    crit = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "    scaler = GradScaler()\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=CFG.LR, steps_per_epoch=len(train_loader), epochs=CFG.EPOCHS)\n",
        "\n",
        "    # 3. Training Loop\n",
        "    best_wer = 1.0\n",
        "    for epoch in range(1, CFG.EPOCHS+1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Ep {epoch}\", leave=False)\n",
        "\n",
        "        for i, (x, y, xl, yl, sids) in enumerate(pbar):\n",
        "            x, y = x.to(CFG.DEVICE), y.to(CFG.DEVICE)\n",
        "            sids = sids.to(CFG.DEVICE)\n",
        "\n",
        "            with autocast('cuda', dtype=torch.bfloat16):\n",
        "                logits = model(x, sids)\n",
        "                loss = crit(logits.permute(1,0,2), y, xl, yl) / CFG.GRAD_ACCUM_STEPS\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % CFG.GRAD_ACCUM_STEPS == 0:\n",
        "                scaler.unscale_(opt)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "                opt.zero_grad()\n",
        "                sched.step()\n",
        "\n",
        "            total_loss += loss.item() * CFG.GRAD_ACCUM_STEPS\n",
        "            pbar.set_postfix(loss=total_loss/(i+1))\n",
        "\n",
        "        # Validation\n",
        "        v_loss, v_wer = validate(model, val_loader)\n",
        "        print(f\"Epoch {epoch}: Train Loss={total_loss/len(train_loader):.3f} | Val Loss={v_loss:.3f} | WER={v_wer:.4f}\")\n",
        "\n",
        "        if v_wer < best_wer:\n",
        "            best_wer = v_wer\n",
        "            torch.save(model.state_dict(), f\"best_model_fold_{fold}.pt\")\n",
        "            print(f\"--> Saved Best WER: {best_wer:.4f}\")\n",
        "\n",
        "    fold_results.append(best_wer)\n",
        "\n",
        "    # Cleanup\n",
        "    del model, opt, scaler, train_loader, val_loader\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\nFinal Results: {fold_results}\")\n",
        "print(f\"Average WER: {np.mean(fold_results):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSja-lG4B9SE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuOfH0KBB9SF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5jSmf0EB9SF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMUWntYaB9SG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}