{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting jiwer\n",
                        "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
                        "Collecting click>=8.1.8 (from jiwer)\n",
                        "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
                        "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
                        "  Downloading rapidfuzz-3.14.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
                        "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
                        "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
                        "Downloading rapidfuzz-3.14.3-cp312-cp312-macosx_11_0_arm64.whl (1.4 MB)\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hInstalling collected packages: rapidfuzz, click, jiwer\n",
                        "\u001b[2K  Attempting uninstall: click\n",
                        "\u001b[2K    Found existing installation: click 8.1.7\n",
                        "\u001b[2K    Uninstalling click-8.1.7:\n",
                        "\u001b[2K      Successfully uninstalled click-8.1.7\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [jiwer]\n",
                        "\u001b[1A\u001b[2KSuccessfully installed click-8.3.1 jiwer-4.0.0 rapidfuzz-3.14.3\n"
                    ]
                }
            ],
            "source": [
                "!pip install jiwer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Optimized Brain-to-Text '25\n",
                "## Phase 1: Setup & Baseline Reproduction\n",
                "This notebook implements the optimized pipeline, starting with the baseline reproduction and moving towards Conformer architecture."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running on device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import yaml\n",
                "import h5py\n",
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from torch import nn\n",
                "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
                "from torch.nn import functional as F\n",
                "import torch.nn.utils.rnn as rnn_utils\n",
                "from tqdm.auto import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import jiwer\n",
                "\n",
                "class CFG:\n",
                "    # --- Model Hyperparameters ---\n",
                "    N_HEAD = 8 \n",
                "    \n",
                "    # --- Training ---\n",
                "    EPOCHS = 5\n",
                "    LR = 1e-3\n",
                "    BATCH_SIZE = 32\n",
                "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "    # --- Paths ---\n",
                "    DATA_DIR = \"/Users/pswmi64/Desktop/Brain-to-Text-25/t15_copyTask_neuralData/hdf5_data_final\"\n",
                "    CHECKPOINT_PATH = \"/Users/pswmi64/Desktop/Brain-to-Text-25/t15_pretrained_rnn_baseline/t15_pretrained_rnn_baseline/checkpoint/best_checkpoint\"\n",
                "    \n",
                "print(f\"Running on device: {CFG.DEVICE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading datasets...\n",
                        "Found 45 session folders.\n",
                        "Warning: File not found /Users/pswmi64/Desktop/Brain-to-Text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.28/data_val.hdf5, creating empty dataset.\n",
                        "Warning: File not found /Users/pswmi64/Desktop/Brain-to-Text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.28/data_test.hdf5, creating empty dataset.\n",
                        "Warning: File not found /Users/pswmi64/Desktop/Brain-to-Text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.25/data_val.hdf5, creating empty dataset.\n",
                        "Warning: File not found /Users/pswmi64/Desktop/Brain-to-Text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.25/data_test.hdf5, creating empty dataset.\n",
                        "Warning: File not found /Users/pswmi64/Desktop/Brain-to-Text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.03/data_val.hdf5, creating empty dataset.\n",
                        "Warning: File not found /Users/pswmi64/Desktop/Brain-to-Text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.03/data_test.hdf5, creating empty dataset.\n",
                        "Warning: File not found /Users/pswmi64/Desktop/Brain-to-Text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11/data_val.hdf5, creating empty dataset.\n",
                        "Warning: File not found /Users/pswmi64/Desktop/Brain-to-Text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11/data_test.hdf5, creating empty dataset.\n",
                        "Train: 8072, Val: 1426, Test: 1450\n"
                    ]
                }
            ],
            "source": [
                "# --- Data Loading Utilities ---\n",
                "\n",
                "def temporal_mask(data, mask_percentage=0.05, mask_value=0.0):\n",
                "    \"\"\"\n",
                "    Applies temporal masking to a 2D tensor [Sequence, Features].\n",
                "    \"\"\"\n",
                "    if not torch.is_tensor(data):\n",
                "        data = torch.tensor(data, dtype=torch.float32)\n",
                "        \n",
                "    seq_len, _ = data.shape\n",
                "    num_to_mask = int(seq_len * mask_percentage)\n",
                "    \n",
                "    if num_to_mask > 0:\n",
                "        mask_indices = torch.randperm(seq_len)[:num_to_mask]\n",
                "        data[mask_indices, :] = mask_value\n",
                "        \n",
                "    return data\n",
                "\n",
                "class BrainDataset(Dataset):\n",
                "    def __init__(self, hdf5_file, input_key=\"input_features\", target_key=\"seq_class_ids\", is_test=False, use_augmentation=False):\n",
                "        self.file_path = hdf5_file\n",
                "        self.input_key = input_key\n",
                "        self.target_key = target_key\n",
                "        self.is_test = is_test\n",
                "        self.use_augmentation = use_augmentation \n",
                "        self.file = None\n",
                "        \n",
                "        try:\n",
                "            with h5py.File(self.file_path, \"r\") as f:\n",
                "                self.trial_keys = sorted(list(f.keys()))\n",
                "        except FileNotFoundError:\n",
                "            print(f\"Warning: File not found {self.file_path}, creating empty dataset.\")\n",
                "            self.trial_keys = []\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.trial_keys)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        if self.file is None:\n",
                "            self.file = h5py.File(self.file_path, \"r\")\n",
                "            \n",
                "        trial_key = self.trial_keys[idx]\n",
                "        trial_group = self.file[trial_key]\n",
                "        \n",
                "        x_data = trial_group[self.input_key][:]\n",
                "        x = torch.tensor(x_data, dtype=torch.float32)\n",
                "        \n",
                "        if self.use_augmentation and not self.is_test:\n",
                "            x = temporal_mask(x, mask_percentage=0.1)\n",
                "        \n",
                "        if self.target_key in trial_group:\n",
                "            y_data = trial_group[self.target_key][:]\n",
                "            y = torch.tensor(y_data, dtype=torch.long)\n",
                "        else:\n",
                "            y = torch.tensor([], dtype=torch.long)\n",
                "        \n",
                "        if self.is_test:\n",
                "            return x, y, trial_key\n",
                "        else:\n",
                "            return x, y\n",
                "\n",
                "def custom_collate(batch):\n",
                "    is_test = len(batch[0]) == 3\n",
                "    if is_test:\n",
                "        xs, ys, keys = zip(*batch)\n",
                "    else:\n",
                "        xs, ys = zip(*batch)\n",
                "        \n",
                "    x_lengths = torch.tensor([len(x) for x in xs], dtype=torch.long)\n",
                "    y_lengths = torch.tensor([len(y) for y in ys], dtype=torch.long)\n",
                "    \n",
                "    padded_xs = rnn_utils.pad_sequence(xs, batch_first=True, padding_value=0.0)\n",
                "    padded_ys = rnn_utils.pad_sequence(ys, batch_first=True, padding_value=0)\n",
                "    \n",
                "    if is_test:\n",
                "        return padded_xs, padded_ys, x_lengths, y_lengths, keys\n",
                "    else:\n",
                "        return padded_xs, padded_ys, x_lengths, y_lengths\n",
                "\n",
                "def load_datasets():\n",
                "    train_datasets = []\n",
                "    val_datasets = []\n",
                "    test_datasets = []\n",
                "\n",
                "    subfolders = [f.path for f in os.scandir(CFG.DATA_DIR) if f.is_dir()]\n",
                "    print(f\"Found {len(subfolders)} session folders.\")\n",
                "    \n",
                "    for subfolder_path in subfolders:\n",
                "        train_file = os.path.join(subfolder_path, \"data_train.hdf5\")\n",
                "        val_file = os.path.join(subfolder_path, \"data_val.hdf5\")\n",
                "        test_file = os.path.join(subfolder_path, \"data_test.hdf5\")\n",
                "\n",
                "        train_set = BrainDataset(train_file, input_key=\"input_features\", target_key=\"seq_class_ids\", is_test=False, use_augmentation=True)\n",
                "        val_set = BrainDataset(val_file, input_key=\"input_features\", target_key=\"seq_class_ids\", is_test=False, use_augmentation=False)\n",
                "        test_set = BrainDataset(test_file, input_key=\"input_features\", target_key=\"seq_class_ids\", is_test=True, use_augmentation=False) \n",
                "        \n",
                "        if len(train_set) > 0: train_datasets.append(train_set)\n",
                "        if len(val_set) > 0: val_datasets.append(val_set)\n",
                "        if len(test_set) > 0: test_datasets.append(test_set)\n",
                "            \n",
                "    return ConcatDataset(train_datasets), ConcatDataset(val_datasets), ConcatDataset(test_datasets)\n",
                "\n",
                "print(\"Loading datasets...\")\n",
                "train_dataset, val_dataset, test_dataset = load_datasets()\n",
                "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
                "val_loader = DataLoader(val_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
                "test_loader = DataLoader(test_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Conformer Model Initialized with 6233641 parameters\n"
                    ]
                }
            ],
            "source": [
                "# --- Conformer Architecture ---\n",
                "\n",
                "class Swish(nn.Module):\n",
                "    def forward(self, x):\n",
                "        return x * torch.sigmoid(x)\n",
                "\n",
                "class FeedForwardModule(nn.Module):\n",
                "    def __init__(self, dim, expansion_factor=4, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.layer_norm = nn.LayerNorm(dim)\n",
                "        self.linear1 = nn.Linear(dim, dim * expansion_factor)\n",
                "        self.swish = Swish()\n",
                "        self.dropout1 = nn.Dropout(dropout)\n",
                "        self.linear2 = nn.Linear(dim * expansion_factor, dim)\n",
                "        self.dropout2 = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: [B, T, C]\n",
                "        out = self.layer_norm(x)\n",
                "        out = self.linear1(out)\n",
                "        out = self.swish(out)\n",
                "        out = self.dropout1(out)\n",
                "        out = self.linear2(out)\n",
                "        out = self.dropout2(out)\n",
                "        return out\n",
                "\n",
                "class ConvolutionModule(nn.Module):\n",
                "    def __init__(self, dim, kernel_size=31, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.layer_norm = nn.LayerNorm(dim)\n",
                "        # Pointwise\n",
                "        self.pointwise_conv1 = nn.Conv1d(dim, dim * 2, kernel_size=1)\n",
                "        self.glu = nn.GLU(dim=1)\n",
                "        # Depthwise\n",
                "        self.depthwise_conv = nn.Conv1d(dim, dim, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, groups=dim)\n",
                "        self.batch_norm = nn.BatchNorm1d(dim)\n",
                "        self.swish = Swish()\n",
                "        # Pointwise\n",
                "        self.pointwise_conv2 = nn.Conv1d(dim, dim, kernel_size=1)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: [B, T, C]\n",
                "        out = self.layer_norm(x)\n",
                "        out = out.transpose(1, 2) # [B, C, T]\n",
                "        \n",
                "        out = self.pointwise_conv1(out)\n",
                "        out = self.glu(out)\n",
                "        out = self.depthwise_conv(out)\n",
                "        out = self.batch_norm(out)\n",
                "        out = self.swish(out)\n",
                "        out = self.pointwise_conv2(out)\n",
                "        out = self.dropout(out)\n",
                "        \n",
                "        out = out.transpose(1, 2) # [B, T, C]\n",
                "        return out\n",
                "\n",
                "class ConformerBlock(nn.Module):\n",
                "    def __init__(self, dim, n_head, conv_kernel_size=31, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.ff1 = FeedForwardModule(dim, dropout=dropout)\n",
                "        self.self_attn_layer_norm = nn.LayerNorm(dim)\n",
                "        self.self_attn = nn.MultiheadAttention(dim, n_head, dropout=dropout, batch_first=True)\n",
                "        self.conv_module = ConvolutionModule(dim, kernel_size=conv_kernel_size, dropout=dropout)\n",
                "        self.ff2 = FeedForwardModule(dim, dropout=dropout)\n",
                "        self.final_layer_norm = nn.LayerNorm(dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: [B, T, C]\n",
                "        # FF1 (Half Step)\n",
                "        x = x + 0.5 * self.ff1(x)\n",
                "        \n",
                "        # Self Attention\n",
                "        residual = x\n",
                "        x_norm = self.self_attn_layer_norm(x)\n",
                "        attn_out, _ = self.self_attn(x_norm, x_norm, x_norm)\n",
                "        x = residual + self.dropout(attn_out)\n",
                "        \n",
                "        # Convolution\n",
                "        x = x + self.conv_module(x)\n",
                "        \n",
                "        # FF2 (Half Step)\n",
                "        x = x + 0.5 * self.ff2(x)\n",
                "        \n",
                "        # Final Norm\n",
                "        x = self.final_layer_norm(x)\n",
                "        return x\n",
                "\n",
                "class ConformerEncoder(nn.Module):\n",
                "    def __init__(self, input_dim, encoder_dim, n_layers, n_head, output_dim):\n",
                "        super().__init__()\n",
                "        self.input_proj = nn.Linear(input_dim, encoder_dim)\n",
                "        self.layers = nn.ModuleList([\n",
                "            ConformerBlock(encoder_dim, n_head) for _ in range(n_layers)\n",
                "        ])\n",
                "        self.output_proj = nn.Linear(encoder_dim, output_dim)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: [B, T, InputDim]\n",
                "        x = self.input_proj(x)\n",
                "        for layer in self.layers:\n",
                "            x = layer(x)\n",
                "        x = self.output_proj(x)\n",
                "        return nn.functional.log_softmax(x, dim=2)\n",
                "\n",
                "# Model Config\n",
                "INPUT_DIM = 512\n",
                "ENCODER_DIM = 256\n",
                "N_LAYERS = 4\n",
                "N_HEAD = 4\n",
                "OUTPUT_DIM = 41 # 40 phonemes + 1 blank\n",
                "\n",
                "model = ConformerEncoder(INPUT_DIM, ENCODER_DIM, N_LAYERS, N_HEAD, OUTPUT_DIM).to(CFG.DEVICE)\n",
                "print(f\"Conformer Model Initialized with {sum(p.numel() for p in model.parameters())} parameters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Training...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "37c94cf93e30480dabb0764384cf711c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Epoch 1 [Train]:   0%|          | 0/253 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# --- Training & Evaluation ---\n",
                "\n",
                "VOCAB = [\n",
                "    'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', \n",
                "    'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW',\n",
                "    'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', \n",
                "    'ZH', '|'\n",
                "]\n",
                "TOKEN_MAP = {i + 1: phoneme for i, phoneme in enumerate(VOCAB)}\n",
                "TOKEN_MAP[0] = \"\"\n",
                "\n",
                "def greedy_decoder(logits, token_map):\n",
                "    pred_indices = torch.argmax(logits, dim=-1)\n",
                "    collapsed_indices = torch.unique_consecutive(pred_indices)\n",
                "    final_indices = [idx.item() for idx in collapsed_indices if idx.item() != 0]\n",
                "    phonemes = [token_map.get(i, \"?\") for i in final_indices]\n",
                "    return \" \".join(phonemes)\n",
                "\n",
                "def train_one_epoch(epoch, model, train_loader, criterion, optimizer):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    for x, y, x_lengths, y_lengths in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\", leave=False):\n",
                "        x, y, x_lengths, y_lengths = x.to(CFG.DEVICE), y.to(CFG.DEVICE), x_lengths.to(CFG.DEVICE), y_lengths.to(CFG.DEVICE)\n",
                "        optimizer.zero_grad()\n",
                "        y_pred = model(x)\n",
                "        y_pred_for_loss = y_pred.permute(1, 0, 2)\n",
                "        loss = criterion(y_pred_for_loss, y, x_lengths, y_lengths)\n",
                "        if torch.isnan(loss) or torch.isinf(loss): continue\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        running_loss += loss.item() * x.size(0)\n",
                "    return running_loss / len(train_loader.dataset)\n",
                "\n",
                "def validate_one_epoch(epoch, model, val_loader, criterion, token_map):\n",
                "    model.eval()\n",
                "    val_loss = 0.0\n",
                "    all_pred = []\n",
                "    all_true = []\n",
                "    with torch.no_grad():\n",
                "        for x, y, x_lengths, y_lengths in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\", leave=False):\n",
                "            x, y, x_lengths, y_lengths = x.to(CFG.DEVICE), y.to(CFG.DEVICE), x_lengths.to(CFG.DEVICE), y_lengths.to(CFG.DEVICE)\n",
                "            y_pred = model(x)\n",
                "            y_pred_for_loss = y_pred.permute(1, 0, 2)\n",
                "            loss = criterion(y_pred_for_loss, y, x_lengths, y_lengths)\n",
                "            val_loss += loss.item() * x.size(0)\n",
                "            \n",
                "            for i in range(x.size(0)):\n",
                "                pred_logits = y_pred[i, :x_lengths[i], :]\n",
                "                true_indices = y[i, :y_lengths[i]]\n",
                "                pred_text = greedy_decoder(pred_logits, token_map)\n",
                "                true_text = \" \".join([token_map.get(idx.item(), \"?\") for idx in true_indices])\n",
                "                all_pred.append(pred_text)\n",
                "                all_true.append(true_text)\n",
                "                \n",
                "    wer = jiwer.wer(all_true, all_pred)\n",
                "    return val_loss / len(val_loader.dataset), wer\n",
                "\n",
                "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.LR)\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "for epoch in range(1, CFG.EPOCHS + 1):\n",
                "    train_loss = train_one_epoch(epoch, model, train_loader, criterion, optimizer)\n",
                "    val_loss, wer = validate_one_epoch(epoch, model, val_loader, criterion, TOKEN_MAP)\n",
                "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | WER: {wer:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
