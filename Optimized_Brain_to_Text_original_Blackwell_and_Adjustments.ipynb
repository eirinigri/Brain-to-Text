{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in ./lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: scipy in ./lib/python3.10/site-packages (1.15.3)\n",
      "Requirement already satisfied: click>=8.1.8 in ./lib/python3.10/site-packages (from jiwer) (8.3.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in ./lib/python3.10/site-packages (from jiwer) (3.14.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in ./lib/python3.10/site-packages (from scipy) (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Brain-to-Text '25\n",
    "## Phases 1, 2 & 3: Signal Processing + Adapter + Beam Search\n",
    "This notebook implements:\n",
    "- **Phase 1:** Gaussian Smoothing, Robust Normalization, Time/Channel Masking\n",
    "- **Phase 2:** Subject-Specific Projection (Adapter Layer)\n",
    "- **Phase 3:** CTC Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data_ssd/ai_workspace/venv_torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# This helps with memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import jiwer\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "class CFG:\n",
    "    N_HEAD = 4 # Keep this lower if needed\n",
    "    EPOCHS = 5\n",
    "    LR = 1e-3\n",
    "    BATCH_SIZE = 8  # <--- Drop this from 32 to 8\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    DATA_DIR = DATA_DIR = r\"/mnt/data_ssd/ai_workspace/venv_torch/t15_copyTask_neuralData/hdf5_data_final\"\n",
    "    CHECKPOINT_PATH = r\"/mnt/data_ssd/ai_workspace/venv_torch/t15_pretrained_rnn_baseline\"\n",
    "    \n",
    "print(f\"Running on device: {CFG.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Signal Processing Functions (Phase 1) ---\n",
    "\n",
    "def gaussian_smoothing_1d(data, sigma=20):\n",
    "    \"\"\"Applies 1D Gaussian smoothing along the time dimension.\"\"\"\n",
    "    return gaussian_filter1d(data, sigma=sigma, axis=0)\n",
    "\n",
    "def robust_scale(data, median, iqr):\n",
    "    \"\"\"Scales data using robust statistics (Median and IQR).\"\"\"\n",
    "    return (data - median) / (iqr + 1e-6)\n",
    "\n",
    "def temporal_mask(data, mask_percentage=0.05, mask_value=0.0):\n",
    "    \"\"\"Applies temporal masking to a 2D tensor [Sequence, Features].\"\"\"\n",
    "    if not torch.is_tensor(data):\n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "    seq_len, _ = data.shape\n",
    "    num_to_mask = int(seq_len * mask_percentage)\n",
    "    if num_to_mask > 0:\n",
    "        mask_indices = torch.randperm(seq_len)[:num_to_mask]\n",
    "        data[mask_indices, :] = mask_value\n",
    "    return data\n",
    "\n",
    "def channel_mask(data, mask_percentage=0.1, mask_value=0.0):\n",
    "    \"\"\"Masks a random subset of channels.\"\"\"\n",
    "    if not torch.is_tensor(data):\n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "    _, n_channels = data.shape\n",
    "    num_to_mask = int(n_channels * mask_percentage)\n",
    "    if num_to_mask > 0:\n",
    "        mask_indices = torch.randperm(n_channels)[:num_to_mask]\n",
    "        data[:, mask_indices] = mask_value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Found 45 session folders.\n",
      "Session mapping: {'t15.2023.08.11': 0, 't15.2023.08.13': 1, 't15.2023.08.18': 2, 't15.2023.08.20': 3, 't15.2023.08.25': 4, 't15.2023.08.27': 5, 't15.2023.09.01': 6, 't15.2023.09.03': 7, 't15.2023.09.24': 8, 't15.2023.09.29': 9, 't15.2023.10.01': 10, 't15.2023.10.06': 11, 't15.2023.10.08': 12, 't15.2023.10.13': 13, 't15.2023.10.15': 14, 't15.2023.10.20': 15, 't15.2023.10.22': 16, 't15.2023.11.03': 17, 't15.2023.11.04': 18, 't15.2023.11.17': 19, 't15.2023.11.19': 20, 't15.2023.11.26': 21, 't15.2023.12.03': 22, 't15.2023.12.08': 23, 't15.2023.12.10': 24, 't15.2023.12.17': 25, 't15.2023.12.29': 26, 't15.2024.02.25': 27, 't15.2024.03.03': 28, 't15.2024.03.08': 29, 't15.2024.03.15': 30, 't15.2024.03.17': 31, 't15.2024.04.25': 32, 't15.2024.04.28': 33, 't15.2024.05.10': 34, 't15.2024.06.14': 35, 't15.2024.07.19': 36, 't15.2024.07.21': 37, 't15.2024.07.28': 38, 't15.2025.01.10': 39, 't15.2025.01.12': 40, 't15.2025.03.14': 41, 't15.2025.03.16': 42, 't15.2025.03.30': 43, 't15.2025.04.13': 44}\n",
      "Warning: File not found /mnt/data_ssd/ai_workspace/venv_torch/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11/data_val.hdf5, creating empty dataset.\n",
      "Warning: File not found /mnt/data_ssd/ai_workspace/venv_torch/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11/data_test.hdf5, creating empty dataset.\n",
      "Warning: File not found /mnt/data_ssd/ai_workspace/venv_torch/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.03/data_val.hdf5, creating empty dataset.\n",
      "Warning: File not found /mnt/data_ssd/ai_workspace/venv_torch/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.03/data_test.hdf5, creating empty dataset.\n",
      "Warning: File not found /mnt/data_ssd/ai_workspace/venv_torch/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.25/data_val.hdf5, creating empty dataset.\n",
      "Warning: File not found /mnt/data_ssd/ai_workspace/venv_torch/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.25/data_test.hdf5, creating empty dataset.\n",
      "Warning: File not found /mnt/data_ssd/ai_workspace/venv_torch/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.28/data_val.hdf5, creating empty dataset.\n",
      "Warning: File not found /mnt/data_ssd/ai_workspace/venv_torch/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.28/data_test.hdf5, creating empty dataset.\n",
      "Train: 8072, Val: 1426, Test: 1450, Sessions: 45\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading with Session IDs (Phase 2) ---\n",
    "\n",
    "SESSION_TO_ID = {}\n",
    "\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, hdf5_file, session_id, input_key=\"input_features\", target_key=\"seq_class_ids\", \n",
    "                 is_test=False, use_augmentation=False, smoothing_sigma=0, robust_stats=None):\n",
    "        self.file_path = hdf5_file\n",
    "        self.session_id = session_id\n",
    "        self.input_key = input_key\n",
    "        self.target_key = target_key\n",
    "        self.is_test = is_test\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.smoothing_sigma = smoothing_sigma\n",
    "        self.robust_stats = robust_stats \n",
    "        self.file = None\n",
    "        \n",
    "        try:\n",
    "            with h5py.File(self.file_path, \"r\") as f:\n",
    "                self.trial_keys = sorted(list(f.keys()))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found {self.file_path}, creating empty dataset.\")\n",
    "            self.trial_keys = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trial_keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.file is None:\n",
    "            self.file = h5py.File(self.file_path, \"r\")\n",
    "            \n",
    "        trial_key = self.trial_keys[idx]\n",
    "        trial_group = self.file[trial_key]\n",
    "        x_data = trial_group[self.input_key][:]\n",
    "        \n",
    "        if self.smoothing_sigma > 0:\n",
    "            x_data = gaussian_smoothing_1d(x_data, sigma=self.smoothing_sigma)\n",
    "        \n",
    "        x = torch.tensor(x_data, dtype=torch.float32)\n",
    "        \n",
    "        if self.robust_stats is not None:\n",
    "            x = robust_scale(x, self.robust_stats['median'], self.robust_stats['iqr'])\n",
    "        \n",
    "        if self.use_augmentation and not self.is_test:\n",
    "            x = temporal_mask(x, mask_percentage=0.1)\n",
    "            x = channel_mask(x, mask_percentage=0.1)\n",
    "        \n",
    "        if self.target_key in trial_group:\n",
    "            y_data = trial_group[self.target_key][:]\n",
    "            y = torch.tensor(y_data, dtype=torch.long)\n",
    "        else:\n",
    "            y = torch.tensor([], dtype=torch.long)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return x, y, self.session_id, trial_key\n",
    "        else:\n",
    "            return x, y, self.session_id\n",
    "\n",
    "def custom_collate(batch):\n",
    "    is_test = len(batch[0]) == 4\n",
    "    if is_test:\n",
    "        xs, ys, session_ids, keys = zip(*batch)\n",
    "    else:\n",
    "        xs, ys, session_ids = zip(*batch)\n",
    "        \n",
    "    x_lengths = torch.tensor([len(x) for x in xs], dtype=torch.long)\n",
    "    y_lengths = torch.tensor([len(y) for y in ys], dtype=torch.long)\n",
    "    session_ids = torch.tensor(session_ids, dtype=torch.long)\n",
    "    \n",
    "    padded_xs = rnn_utils.pad_sequence(xs, batch_first=True, padding_value=0.0)\n",
    "    padded_ys = rnn_utils.pad_sequence(ys, batch_first=True, padding_value=0)\n",
    "    \n",
    "    if is_test:\n",
    "        return padded_xs, padded_ys, x_lengths, y_lengths, session_ids, keys\n",
    "    else:\n",
    "        return padded_xs, padded_ys, x_lengths, y_lengths, session_ids\n",
    "\n",
    "def load_datasets(smoothing_sigma=20):\n",
    "    global SESSION_TO_ID\n",
    "    train_datasets, val_datasets, test_datasets = [], [], []\n",
    "\n",
    "    subfolders = sorted([f.path for f in os.scandir(CFG.DATA_DIR) if f.is_dir()])\n",
    "    print(f\"Found {len(subfolders)} session folders.\")\n",
    "    \n",
    "    SESSION_TO_ID = {os.path.basename(path): i for i, path in enumerate(subfolders)}\n",
    "    print(f\"Session mapping: {SESSION_TO_ID}\")\n",
    "    \n",
    "    robust_stats = None\n",
    "    \n",
    "    for subfolder_path in subfolders:\n",
    "        session_name = os.path.basename(subfolder_path)\n",
    "        session_id = SESSION_TO_ID[session_name]\n",
    "        \n",
    "        train_set = BrainDataset(os.path.join(subfolder_path, \"data_train.hdf5\"), session_id, \n",
    "                                 is_test=False, use_augmentation=True, smoothing_sigma=smoothing_sigma, robust_stats=robust_stats)\n",
    "        val_set = BrainDataset(os.path.join(subfolder_path, \"data_val.hdf5\"), session_id, \n",
    "                               is_test=False, use_augmentation=False, smoothing_sigma=smoothing_sigma, robust_stats=robust_stats)\n",
    "        test_set = BrainDataset(os.path.join(subfolder_path, \"data_test.hdf5\"), session_id, \n",
    "                                is_test=True, use_augmentation=False, smoothing_sigma=smoothing_sigma, robust_stats=robust_stats)\n",
    "        \n",
    "        if len(train_set) > 0: train_datasets.append(train_set)\n",
    "        if len(val_set) > 0: val_datasets.append(val_set)\n",
    "        if len(test_set) > 0: test_datasets.append(test_set)\n",
    "            \n",
    "    return ConcatDataset(train_datasets), ConcatDataset(val_datasets), ConcatDataset(test_datasets), len(SESSION_TO_ID)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset, val_dataset, test_dataset, NUM_SESSIONS = load_datasets(smoothing_sigma=20)\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}, Sessions: {NUM_SESSIONS}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectAdapter(nn.Module):\n",
    "    def __init__(self, input_dim, num_sessions):\n",
    "        super().__init__()\n",
    "        # Use a 3D weight tensor for all sessions at once\n",
    "        self.weight = nn.Parameter(torch.stack([torch.eye(input_dim) for _ in range(num_sessions)]))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_sessions, input_dim))\n",
    "    \n",
    "    def forward(self, x, session_ids):\n",
    "        # x: [Batch, Seq, Dim]\n",
    "        # session_ids: [Batch]\n",
    "        w = self.weight[session_ids] # [Batch, Dim, Dim]\n",
    "        b = self.bias[session_ids].unsqueeze(1) # [Batch, 1, Dim]\n",
    "        \n",
    "        # Process whole batch with one matrix multiplication\n",
    "        return torch.bmm(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, dim, expansion_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.linear1 = nn.Linear(dim, dim * expansion_factor)\n",
    "        self.swish = Swish()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim * expansion_factor, dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_norm(x)\n",
    "        out = self.linear1(out)\n",
    "        out = self.swish(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.dropout2(out)\n",
    "        return out\n",
    "\n",
    "class ConvolutionModule(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=31, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.pointwise_conv1 = nn.Conv1d(dim, dim * 2, kernel_size=1)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        self.depthwise_conv = nn.Conv1d(dim, dim, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, groups=dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(dim)\n",
    "        self.swish = Swish()\n",
    "        self.pointwise_conv2 = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_norm(x)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.pointwise_conv1(out)\n",
    "        out = self.glu(out)\n",
    "        out = self.depthwise_conv(out)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.swish(out)\n",
    "        out = self.pointwise_conv2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out.transpose(1, 2)\n",
    "        return out\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, dim, n_head, conv_kernel_size=31, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ff1 = FeedForwardModule(dim, dropout=dropout)\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(dim)\n",
    "        # We no longer use nn.MultiheadAttention to ensure Flash Attention 2 is used\n",
    "        self.n_head = n_head\n",
    "        self.dropout_p = dropout\n",
    "        self.conv_module = ConvolutionModule(dim, kernel_size=conv_kernel_size, dropout=dropout)\n",
    "        self.ff2 = FeedForwardModule(dim, dropout=dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + 0.5 * self.ff1(x)\n",
    "        residual = x\n",
    "        x_norm = self.self_attn_layer_norm(x)\n",
    "        \n",
    "        # Blackwell Optimization: Scaled Dot Product Attention (Flash Attention)\n",
    "        # This prevents the 2.8GB allocation by calculating attention in blocks\n",
    "        attn_out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            x_norm, x_norm, x_norm, \n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "            is_causal=False\n",
    "        )\n",
    "        \n",
    "        x = residual + self.dropout(attn_out)\n",
    "        x = x + self.conv_module(x)\n",
    "        x = x + 0.5 * self.ff2(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class ConformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoder_dim, n_layers, n_head, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, encoder_dim)\n",
    "        self.layers = nn.ModuleList([ConformerBlock(encoder_dim, n_head) for _ in range(n_layers)])\n",
    "        self.output_proj = nn.Linear(encoder_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        for layer in self.layers:\n",
    "            # Gradient Checkpointing: Trades 20% speed for 60-70% VRAM saving\n",
    "            x = checkpoint(layer, x, use_reentrant=False)\n",
    "        x = self.output_proj(x)\n",
    "        return nn.functional.log_softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrainToTextModel initialized with 17000489 parameters\n"
     ]
    }
   ],
   "source": [
    "# --- Full Model with Subject Adapter (Phase 2) ---\n",
    "\n",
    "class BrainToTextModel(nn.Module):\n",
    "    def __init__(self, input_dim, encoder_dim, n_layers, n_head, output_dim, num_sessions):\n",
    "        super().__init__()\n",
    "        self.adapter = SubjectAdapter(input_dim, num_sessions)\n",
    "        self.encoder = ConformerEncoder(input_dim, encoder_dim, n_layers, n_head, output_dim)\n",
    "    \n",
    "    def forward(self, x, session_ids):\n",
    "        x = self.adapter(x, session_ids)\n",
    "        return self.encoder(x)\n",
    "\n",
    "INPUT_DIM = 512\n",
    "ENCODER_DIM = 256\n",
    "N_LAYERS = 4\n",
    "N_HEAD = 4\n",
    "OUTPUT_DIM = 41  # 40 phonemes + 1 blank\n",
    "\n",
    "model = BrainToTextModel(INPUT_DIM, ENCODER_DIM, N_LAYERS, N_HEAD, OUTPUT_DIM, NUM_SESSIONS).to(CFG.DEVICE)\n",
    "print(f\"BrainToTextModel initialized with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Beam Search Decoder defined.\n"
     ]
    }
   ],
   "source": [
    "# --- CTC Beam Search Decoder (Phase 3) ---\n",
    "\n",
    "def ctc_beam_search(log_probs, beam_width=10, blank_id=0):\n",
    "    \"\"\"\n",
    "    CTC Prefix Beam Search Decoder.\n",
    "    \n",
    "    Args:\n",
    "        log_probs: [T, num_classes] numpy array of log probabilities\n",
    "        beam_width: Number of beams to keep\n",
    "        blank_id: ID of the blank token (usually 0)\n",
    "    \n",
    "    Returns:\n",
    "        Best decoded sequence (list of token IDs, excluding blanks)\n",
    "    \"\"\"\n",
    "    T, num_classes = log_probs.shape\n",
    "    \n",
    "    # Each beam is: (prefix_tuple, (log_prob_blank, log_prob_non_blank))\n",
    "    # prefix_tuple: the decoded sequence so far\n",
    "    # log_prob_blank: prob of this prefix ending in blank\n",
    "    # log_prob_non_blank: prob of this prefix ending in non-blank\n",
    "    \n",
    "    NEG_INF = float('-inf')\n",
    "    beams = {(): (0.0, NEG_INF)}  # Start with empty prefix, prob=1 for blank path\n",
    "    \n",
    "    for t in range(T):\n",
    "        new_beams = defaultdict(lambda: (NEG_INF, NEG_INF))\n",
    "        \n",
    "        for prefix, (pb, pnb) in beams.items():\n",
    "            # Total probability of this prefix\n",
    "            p_prefix = np.logaddexp(pb, pnb)\n",
    "            \n",
    "            for c in range(num_classes):\n",
    "                p_c = log_probs[t, c]\n",
    "                \n",
    "                if c == blank_id:\n",
    "                    # Extend with blank - prefix stays same\n",
    "                    new_pb, new_pnb = new_beams[prefix]\n",
    "                    new_pb = np.logaddexp(new_pb, p_prefix + p_c)\n",
    "                    new_beams[prefix] = (new_pb, new_pnb)\n",
    "                else:\n",
    "                    # Extend with non-blank character\n",
    "                    new_prefix = prefix + (c,)\n",
    "                    \n",
    "                    # Case 1: Previous was blank OR different character\n",
    "                    new_pb, new_pnb = new_beams[new_prefix]\n",
    "                    new_pnb = np.logaddexp(new_pnb, p_prefix + p_c)\n",
    "                    \n",
    "                    # Case 2: Repeat character (only if last char is same)\n",
    "                    if len(prefix) > 0 and prefix[-1] == c:\n",
    "                        # Can only extend via blank path (pb)\n",
    "                        new_pnb = np.logaddexp(new_pnb, pb + p_c)\n",
    "                        # Also, staying at same prefix\n",
    "                        old_pb, old_pnb = new_beams[prefix]\n",
    "                        old_pnb = np.logaddexp(old_pnb, pnb + p_c)\n",
    "                        new_beams[prefix] = (old_pb, old_pnb)\n",
    "                    \n",
    "                    new_beams[new_prefix] = (new_pb, new_pnb)\n",
    "        \n",
    "        # Prune to beam_width\n",
    "        scored = [(np.logaddexp(pb, pnb), prefix) for prefix, (pb, pnb) in new_beams.items()]\n",
    "        scored.sort(reverse=True)\n",
    "        beams = {prefix: new_beams[prefix] for _, prefix in scored[:beam_width]}\n",
    "    \n",
    "    # Return best prefix\n",
    "    best_prefix = max(beams.keys(), key=lambda p: np.logaddexp(*beams[p]))\n",
    "    return list(best_prefix)\n",
    "\n",
    "def beam_search_decoder(logits, token_map, beam_width=10):\n",
    "    \"\"\"\n",
    "    Decode logits using CTC beam search.\n",
    "    \n",
    "    Args:\n",
    "        logits: [T, num_classes] tensor (log probabilities)\n",
    "        token_map: dict mapping token IDs to phoneme strings\n",
    "        beam_width: Number of beams\n",
    "    \n",
    "    Returns:\n",
    "        Decoded phoneme string\n",
    "    \"\"\"\n",
    "    log_probs = logits.cpu().numpy()\n",
    "    decoded_ids = ctc_beam_search(log_probs, beam_width=beam_width, blank_id=0)\n",
    "    phonemes = [token_map.get(i, \"?\") for i in decoded_ids]\n",
    "    return \" \".join(phonemes)\n",
    "\n",
    "print(\"CTC Beam Search Decoder defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing beam search decoder...\n",
      "Greedy: HH D N L OW Z AE AO B ER D NG V UH S OW AW HH ZH U...\n",
      "Beam:   HH D N L OW Z AE AO B ER D NG V UH S OW AW HH ZH U...\n",
      "Beam search decoder working!\n"
     ]
    }
   ],
   "source": [
    "# --- Decoders: Greedy vs Beam Search ---\n",
    "\n",
    "VOCAB = [\n",
    "    'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', \n",
    "    'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW',\n",
    "    'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', \n",
    "    'ZH', '|'\n",
    "]\n",
    "TOKEN_MAP = {i + 1: phoneme for i, phoneme in enumerate(VOCAB)}\n",
    "TOKEN_MAP[0] = \"\"  # blank\n",
    "\n",
    "def greedy_decoder(logits, token_map):\n",
    "    \"\"\"Simple greedy CTC decoder (baseline).\"\"\"\n",
    "    pred_indices = torch.argmax(logits, dim=-1)\n",
    "    collapsed_indices = torch.unique_consecutive(pred_indices)\n",
    "    final_indices = [idx.item() for idx in collapsed_indices if idx.item() != 0]\n",
    "    phonemes = [token_map.get(i, \"?\") for i in final_indices]\n",
    "    return \" \".join(phonemes)\n",
    "\n",
    "# --- Test Beam Search ---\n",
    "print(\"Testing beam search decoder...\")\n",
    "dummy_logits = torch.randn(50, 41)  # 50 timesteps, 41 classes\n",
    "dummy_logits = F.log_softmax(dummy_logits, dim=-1)\n",
    "\n",
    "greedy_result = greedy_decoder(dummy_logits, TOKEN_MAP)\n",
    "beam_result = beam_search_decoder(dummy_logits, TOKEN_MAP, beam_width=5)\n",
    "\n",
    "print(f\"Greedy: {greedy_result[:50]}...\")\n",
    "print(f\"Beam:   {beam_result[:50]}...\")\n",
    "print(\"Beam search decoder working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "def train_one_epoch(epoch, model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for x, y, x_lengths, y_lengths, session_ids in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\", leave=False):\n",
    "        x = x.to(CFG.DEVICE)\n",
    "        y = y.to(CFG.DEVICE)\n",
    "        x_lengths = x_lengths.to(CFG.DEVICE)\n",
    "        y_lengths = y_lengths.to(CFG.DEVICE)\n",
    "        session_ids = session_ids.to(CFG.DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # BF16 is native to Blackwell and uses half the memory of FP32\n",
    "        with autocast('cuda', dtype=torch.bfloat16):\n",
    "            y_pred = model(x, session_ids)\n",
    "            y_pred_for_loss = y_pred.permute(1, 0, 2)\n",
    "            loss = criterion(y_pred_for_loss, y, x_lengths, y_lengths)\n",
    "            \n",
    "        if torch.isnan(loss) or torch.isinf(loss): continue\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    return running_loss / len(train_loader.dataset)\n",
    "\n",
    "def validate_one_epoch(epoch, model, val_loader, criterion, token_map, use_beam_search=False, beam_width=10):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_pred, all_true = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast('cuda', dtype=torch.bfloat16):\n",
    "            for x, y, x_lengths, y_lengths, session_ids in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\", leave=False):\n",
    "                x = x.to(CFG.DEVICE)\n",
    "                y = y.to(CFG.DEVICE)\n",
    "                x_lengths = x_lengths.to(CFG.DEVICE)\n",
    "                y_lengths = y_lengths.to(CFG.DEVICE)\n",
    "                session_ids = session_ids.to(CFG.DEVICE)\n",
    "                \n",
    "                y_pred = model(x, session_ids)\n",
    "                y_pred_for_loss = y_pred.permute(1, 0, 2)\n",
    "                loss = criterion(y_pred_for_loss, y, x_lengths, y_lengths)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "                \n",
    "                # Move to CPU for decoding\n",
    "                y_pred_cpu = y_pred.to(torch.float32).cpu()\n",
    "                \n",
    "                for i in range(x.size(0)):\n",
    "                    pred_logits = y_pred_cpu[i, :x_lengths[i], :]\n",
    "                    true_indices = y[i, :y_lengths[i]]\n",
    "                    \n",
    "                    if use_beam_search:\n",
    "                        pred_text = beam_search_decoder(pred_logits, token_map, beam_width)\n",
    "                    else:\n",
    "                        pred_text = greedy_decoder(pred_logits, token_map)\n",
    "                        \n",
    "                    true_text = \" \".join([token_map.get(idx.item(), \"?\") for idx in true_indices])\n",
    "                    all_pred.append(pred_text)\n",
    "                    all_true.append(true_text)\n",
    "                \n",
    "    wer = jiwer.wer(all_true, all_pred)\n",
    "    return val_loss / len(val_loader.dataset), wer\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training (Blackwell Optimized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: -0.2677 | Val Loss: -0.3312 | WER: 0.9504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: -0.2789 | Val Loss: -0.3355 | WER: 0.8265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: -0.2804 | Val Loss: -0.3368 | WER: 0.7219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: -0.2810 | Val Loss: -0.3303 | WER: 0.7274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: -0.2821 | Val Loss: -0.3295 | WER: 0.7621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 1. Clear GPU memory manually\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# 2. Re-init loader with safe Batch Size for 16GB VRAM\n",
    "CFG.BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "print(\"Starting Training (Blackwell Optimized)...\")\n",
    "for epoch in range(1, CFG.EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(epoch, model, train_loader, criterion, optimizer)\n",
    "    val_loss, wer = validate_one_epoch(epoch, model, val_loader, criterion, TOKEN_MAP, use_beam_search=False)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | WER: {wer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Greedy vs Beam Search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy WER: 0.7621\n",
      "Beam (k=10) WER: 0.7676\n",
      "Improvement: -0.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# --- Compare Greedy vs Beam Search WER ---\n",
    "# Run this after training to compare decoding strategies:\n",
    "\n",
    "print(\"Comparing Greedy vs Beam Search...\")\n",
    "_, wer_greedy = validate_one_epoch(0, model, val_loader, criterion, TOKEN_MAP, use_beam_search=False)\n",
    "_, wer_beam = validate_one_epoch(0, model, val_loader, criterion, TOKEN_MAP, use_beam_search=True, beam_width=10)\n",
    "print(f\"Greedy WER: {wer_greedy:.4f}\")\n",
    "print(f\"Beam (k=10) WER: {wer_beam:.4f}\")\n",
    "print(f\"Improvement: {(wer_greedy - wer_beam) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Blackwell RTX 5060 Ti)",
   "language": "python",
   "name": "venv_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
